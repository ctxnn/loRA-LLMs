{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31merror: externally-managed-environment\n",
      "\u001b[1;31m\n",
      "\u001b[1;31m× This environment is externally managed\n",
      "\u001b[1;31m╰─> To install Python packages system-wide, try brew install\n",
      "\u001b[1;31m    xyz, where xyz is the package you are trying to\n",
      "\u001b[1;31m    install.\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    If you wish to install a Python library that isn't in Homebrew,\n",
      "\u001b[1;31m    use a virtual environment:\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    python3 -m venv path/to/venv\n",
      "\u001b[1;31m    source path/to/venv/bin/activate\n",
      "\u001b[1;31m    python3 -m pip install xyz\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    If you wish to install a Python application that isn't in Homebrew,\n",
      "\u001b[1;31m    it may be easiest to use 'pipx install xyz', which will manage a\n",
      "\u001b[1;31m    virtual environment for you. You can install pipx with\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    brew install pipx\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    You may restore the old behavior of pip by passing\n",
      "\u001b[1;31m    the '--break-system-packages' flag to pip, or by adding\n",
      "\u001b[1;31m    'break-system-packages = true' to your pip.conf file. The latter\n",
      "\u001b[1;31m    will permanently disable this error.\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    If you disable this error, we STRONGLY recommend that you additionally\n",
      "\u001b[1;31m    pass the '--user' flag to pip, or set 'user = true' in your pip.conf\n",
      "\u001b[1;31m    file. Failure to do this can result in a broken Homebrew installation.\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    Read more about this behavior here: <https://peps.python.org/pep-0668/>\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mnote: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;31mhint: See PEP 668 for the detailed specification. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "from torchvision import datasets \n",
    "from torchvision.transforms import transforms \n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\t1.\tUnder-train full model → find weak class\n",
    "\t2.\tTargeted LoRA fine-tune → fix only that class\n",
    "\t3.\tKeep base weights unchanged → plug-and-play adaptability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "# Temporarily disable SSL certificate verification\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "transform = transforms.ToTensor() \n",
    "train_dataset = datasets.MNIST(root='./data',train=True,transform=transform,download=True)\n",
    "test_dataset = datasets.MNIST(root='./data',train=False,transform=transform,download=True)\n",
    "train_loader = DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIGnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(28*28, 1000)\n",
    "        self.linear2 = nn.Linear(1000, 2000)\n",
    "        self.linear3 = nn.Linear(2000, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.relu(self.linear(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BIGnet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, device, num_epochs, total_iterations_limit, criterion):\n",
    "    total_iterations = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        \n",
    "        total = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            total_iterations += 1\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
    "                return\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = 100. * correct / total\n",
    "        print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {train_loss:.3f}, Accuracy: {train_acc:.2f}%')\n",
    "    return train_loss, train_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1, Loss: 0.314, Accuracy: 91.16%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3141168975392977, 91.15833333333333)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train(model = model,optimizer=optimizer,criterion=criterion,train_loader=train_loader,num_epochs=1,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying the parameters from the original model which we will use later \n",
    "original_weights = {}\n",
    "for name, param in model.named_parameters():\n",
    "    original_weights[name] = param.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear.weight': tensor([[ 0.0279,  0.0322,  0.0298,  ..., -0.0158,  0.0351,  0.0303],\n",
       "         [ 0.0206,  0.0348, -0.0100,  ...,  0.0064, -0.0168, -0.0191],\n",
       "         [-0.0001, -0.0043, -0.0311,  ..., -0.0263,  0.0251,  0.0039],\n",
       "         ...,\n",
       "         [-0.0200, -0.0323, -0.0332,  ..., -0.0112,  0.0125,  0.0234],\n",
       "         [-0.0290, -0.0022,  0.0127,  ..., -0.0173,  0.0035,  0.0124],\n",
       "         [-0.0080, -0.0253,  0.0223,  ...,  0.0341, -0.0339,  0.0051]],\n",
       "        device='mps:0'),\n",
       " 'linear.bias': tensor([-1.2583e-02,  3.0831e-02,  4.5020e-02,  1.4790e-02,  3.0004e-03,\n",
       "          3.2661e-02, -2.6347e-02, -8.0484e-03,  2.0411e-02,  3.8631e-03,\n",
       "          4.0270e-02,  3.2066e-02, -2.3868e-02,  1.6084e-02, -9.5288e-03,\n",
       "         -1.2936e-02,  3.6315e-02,  4.7068e-03,  1.3151e-03,  2.0537e-02,\n",
       "         -8.4143e-03,  2.2198e-04,  3.4289e-02, -3.1297e-02, -1.5063e-02,\n",
       "          1.8455e-02,  1.9217e-02,  3.8654e-03, -1.9805e-02, -1.5384e-02,\n",
       "          2.0541e-02,  2.3218e-02,  3.3477e-02,  1.1731e-02, -2.4781e-02,\n",
       "          2.3861e-02,  3.5226e-02,  1.4390e-02,  1.0673e-02, -1.2524e-02,\n",
       "         -6.4127e-03,  1.9743e-02,  3.0743e-02, -1.4481e-02,  2.5422e-02,\n",
       "         -9.4625e-03, -8.9852e-03, -2.1680e-02,  1.6974e-02,  4.2846e-02,\n",
       "          1.0717e-02,  1.0842e-02, -1.0168e-02,  3.4506e-02,  9.8801e-03,\n",
       "         -2.7090e-03, -5.0244e-03,  2.0419e-02,  1.6623e-02, -2.1618e-02,\n",
       "         -1.5587e-02, -1.7415e-02,  2.3535e-02, -2.0796e-03, -5.2296e-03,\n",
       "          3.3957e-02, -2.9260e-02, -2.1404e-02, -3.0539e-02,  3.4383e-02,\n",
       "          5.9215e-03,  2.9063e-02,  1.7422e-02, -3.3118e-02,  7.2837e-03,\n",
       "          6.0391e-03, -1.1623e-02, -2.5582e-02,  3.8942e-03,  1.0400e-03,\n",
       "          3.3532e-03,  1.1398e-02,  5.4140e-03, -6.5737e-03, -1.1493e-02,\n",
       "         -1.2539e-02,  3.1530e-02,  2.8776e-02,  1.9910e-02, -1.6419e-02,\n",
       "         -1.0254e-03,  1.8442e-02,  3.2160e-02, -2.5321e-02,  3.1368e-02,\n",
       "          3.3393e-02, -8.4911e-04,  3.4472e-02,  3.3877e-02, -5.8029e-03,\n",
       "          6.4519e-03,  5.6670e-03, -2.4149e-02,  3.1046e-02,  9.0056e-03,\n",
       "          1.8694e-02,  5.1770e-03,  1.4544e-02,  6.3285e-03, -1.9788e-02,\n",
       "         -1.4035e-02,  3.3489e-03,  3.8431e-02,  3.6076e-03, -2.4774e-02,\n",
       "          8.8950e-03,  9.8514e-03, -3.8494e-03, -2.6707e-02,  1.3620e-02,\n",
       "         -1.9132e-02, -9.6120e-04, -1.9047e-02, -9.7173e-05,  9.9457e-03,\n",
       "         -2.6857e-02, -1.2357e-02,  1.6150e-02, -1.4217e-02,  3.5827e-02,\n",
       "         -1.6491e-02, -3.9860e-03, -2.6435e-02,  1.2816e-02, -1.1015e-02,\n",
       "          3.4982e-02,  3.8346e-02,  3.4290e-02, -3.2003e-04, -3.4253e-02,\n",
       "          2.0385e-02,  7.0504e-03,  3.5131e-03,  1.1083e-02,  3.5885e-02,\n",
       "         -1.6071e-02,  8.7675e-04,  6.7550e-03, -1.1860e-02, -2.1244e-02,\n",
       "         -5.3822e-03, -1.9152e-03,  2.3011e-02, -3.3109e-02,  6.1597e-03,\n",
       "         -9.0810e-03,  2.4887e-03,  2.7747e-02,  4.2654e-03, -7.8668e-03,\n",
       "          2.0085e-02,  3.8962e-02,  3.1797e-04, -2.7172e-02, -7.9369e-03,\n",
       "         -1.6095e-02,  4.0162e-02,  1.1473e-02, -3.0978e-03,  1.9195e-02,\n",
       "          5.2611e-04, -3.9205e-03,  2.8362e-02, -5.4882e-03,  2.8855e-02,\n",
       "          1.9914e-02, -1.6348e-02,  2.1846e-02, -2.2858e-02,  2.6935e-03,\n",
       "         -1.2303e-02,  1.7330e-02, -2.7611e-02,  3.5551e-03,  5.8983e-03,\n",
       "          7.6152e-03,  6.4728e-03,  3.8676e-02,  2.7475e-02,  1.5190e-02,\n",
       "         -1.2071e-02,  4.8707e-03,  1.9361e-02,  5.2166e-03,  2.6426e-02,\n",
       "         -1.8978e-02,  2.6656e-02,  3.1378e-02, -2.0753e-03,  2.2855e-02,\n",
       "         -2.7326e-03, -7.1179e-03, -1.1120e-02, -3.8808e-02, -2.4211e-02,\n",
       "          2.7552e-02,  2.2200e-02, -2.0073e-03,  4.6514e-02, -1.9993e-02,\n",
       "          1.7971e-02,  3.4237e-02,  2.0442e-02,  1.0257e-02, -2.4462e-02,\n",
       "         -3.0273e-02,  3.9769e-03,  4.0107e-03,  3.0387e-02, -2.2223e-02,\n",
       "          3.3080e-02,  1.1038e-02,  4.0046e-02,  6.5540e-03,  2.4875e-03,\n",
       "          3.6213e-02,  1.6205e-03, -2.5083e-02, -4.6905e-03, -1.8295e-02,\n",
       "          2.5491e-02,  2.5651e-03,  2.2936e-02, -2.1169e-02, -2.3978e-02,\n",
       "         -1.3444e-02, -1.8785e-03,  2.9163e-02,  1.3230e-02,  2.7639e-02,\n",
       "          2.3574e-02, -2.7194e-02,  1.1456e-02,  3.2455e-02, -2.7709e-02,\n",
       "          1.8724e-02,  3.5267e-02, -2.6210e-02, -9.1588e-03,  2.3104e-02,\n",
       "          2.6468e-02,  3.2893e-02,  3.3289e-02, -6.7986e-03,  1.1739e-02,\n",
       "         -1.7223e-02, -1.3574e-02, -2.0389e-02,  2.8107e-02, -1.8787e-02,\n",
       "          4.1706e-02,  2.4364e-02, -1.4087e-02, -1.0751e-02, -1.2817e-02,\n",
       "         -1.2062e-02, -2.8257e-02, -1.0888e-03,  1.0482e-02, -3.0256e-02,\n",
       "          3.7148e-02,  1.6532e-02,  3.6193e-02, -7.9673e-03,  3.0485e-02,\n",
       "          2.2895e-02,  2.0672e-02,  3.9898e-02, -1.5091e-02,  1.2151e-02,\n",
       "         -1.5833e-02,  9.0160e-03,  1.1587e-02,  3.8509e-02, -1.7501e-02,\n",
       "         -3.3761e-02, -6.4656e-04, -2.1245e-02,  3.7733e-02,  3.9057e-02,\n",
       "          7.4281e-03, -8.7518e-03, -1.4801e-02,  2.1274e-02, -1.6884e-02,\n",
       "          1.4104e-02, -2.1263e-02, -4.2574e-03, -2.0443e-02,  2.9388e-02,\n",
       "          1.1736e-02,  3.3421e-02,  4.1832e-03,  3.1275e-03, -1.8609e-02,\n",
       "         -1.9142e-03, -6.8736e-03,  2.3853e-02, -3.2480e-03, -5.2021e-03,\n",
       "         -8.1787e-03, -6.1652e-03, -1.0763e-02, -1.0901e-02,  3.2456e-02,\n",
       "          3.6488e-02, -2.2310e-02,  4.0014e-02,  2.9449e-02,  3.0860e-02,\n",
       "         -2.3570e-02, -1.6218e-02,  4.1497e-03, -1.4147e-02, -3.6934e-02,\n",
       "          1.2551e-02,  5.4589e-03, -5.7317e-04,  9.2366e-03,  1.0217e-02,\n",
       "          1.4015e-02,  1.8798e-02, -1.7588e-02,  2.2657e-02,  1.3853e-03,\n",
       "          6.0153e-03,  3.2072e-02,  1.2897e-02, -1.9555e-02, -3.6114e-03,\n",
       "          1.8900e-02,  1.3216e-05,  2.5859e-03, -1.4392e-02,  7.9026e-03,\n",
       "          2.8130e-02, -7.4305e-03, -4.2663e-03,  6.7422e-03, -2.1121e-02,\n",
       "         -1.2046e-02, -3.1042e-02,  5.5570e-03,  2.3836e-02, -2.6646e-03,\n",
       "         -1.6783e-03,  2.9224e-02, -4.2468e-04,  3.5272e-02,  3.3191e-02,\n",
       "         -2.1157e-02,  2.3840e-02,  2.6442e-02, -2.3396e-02, -1.2547e-02,\n",
       "         -2.2732e-02, -4.1822e-02,  5.0659e-03,  4.2640e-02,  4.1798e-03,\n",
       "         -4.1173e-03,  3.2062e-02, -1.6841e-02, -1.1040e-02,  4.2952e-02,\n",
       "          8.5483e-03,  1.1366e-03, -6.7931e-03, -1.6515e-03, -2.0457e-02,\n",
       "          3.3063e-02,  2.1018e-02,  8.8016e-03,  2.6176e-02, -1.3197e-02,\n",
       "          2.1177e-02,  3.0420e-04,  2.2638e-02, -1.7316e-02,  2.7219e-02,\n",
       "         -1.4724e-02,  3.7113e-03,  2.2422e-02, -2.2217e-02,  1.5326e-03,\n",
       "          3.5233e-02, -5.4864e-03,  6.7095e-03, -1.4656e-02, -1.1340e-02,\n",
       "         -1.7204e-02,  2.1198e-02, -2.8009e-02,  2.6116e-02, -1.2558e-02,\n",
       "          1.4616e-02,  3.6610e-03, -3.8579e-03,  1.7256e-02,  8.0134e-03,\n",
       "         -2.0941e-02,  3.7321e-03,  4.0326e-02, -2.8557e-02, -2.7573e-02,\n",
       "          1.9468e-02,  4.5260e-02, -1.1287e-04, -2.1939e-02, -4.5029e-03,\n",
       "         -2.2320e-02,  2.0480e-02,  2.0822e-02,  4.9834e-02,  2.2054e-02,\n",
       "          2.5295e-02, -2.2426e-02, -8.4389e-03, -3.6841e-02, -2.5523e-02,\n",
       "         -2.7318e-02, -2.2352e-02, -2.5358e-03, -1.2275e-02, -2.9194e-02,\n",
       "         -5.7754e-04, -2.0647e-02,  3.7332e-02,  8.1910e-03,  1.3247e-02,\n",
       "          1.2971e-02,  1.8444e-02,  8.5613e-04,  3.3617e-02,  1.9810e-03,\n",
       "          5.2870e-03, -1.1349e-02, -3.0959e-03, -6.5204e-04,  1.2122e-02,\n",
       "          4.3799e-02, -3.0102e-02,  1.8235e-02, -1.0528e-02,  3.1854e-02,\n",
       "         -2.8101e-02,  3.1285e-02,  3.2268e-02,  2.9697e-03,  3.7266e-02,\n",
       "          3.4662e-02,  3.3717e-02, -2.7787e-02, -2.4361e-02, -9.2324e-03,\n",
       "         -2.8088e-02, -8.0450e-03,  5.9594e-03,  2.1447e-02, -2.6789e-02,\n",
       "          3.5836e-02, -6.7649e-03,  2.6006e-02,  2.3233e-02,  5.0370e-03,\n",
       "          1.9015e-02,  1.1548e-03,  1.7890e-02, -2.3959e-02,  6.7749e-03,\n",
       "          1.4260e-02,  1.2009e-02, -1.3515e-02,  8.8920e-04, -2.2847e-02,\n",
       "          3.3167e-02, -2.0791e-02,  3.3165e-02,  8.4313e-03,  3.9014e-02,\n",
       "          3.9791e-02,  1.8354e-02,  1.0758e-02,  2.5691e-02,  1.7151e-02,\n",
       "          2.3790e-02,  1.3005e-02, -8.4036e-05, -6.4120e-03,  1.3487e-03,\n",
       "          3.4695e-02,  3.4081e-02,  3.9724e-02, -1.5867e-02, -9.9592e-03,\n",
       "          1.0112e-02, -1.3316e-03,  2.3671e-05,  2.5951e-02, -2.4691e-02,\n",
       "          3.6410e-02,  9.1851e-03, -1.7843e-02,  5.9382e-03,  1.4662e-02,\n",
       "          3.0970e-02, -1.9909e-03,  1.7317e-02,  2.2678e-02, -9.0137e-04,\n",
       "          3.3728e-02, -8.7539e-03, -2.9865e-02, -3.3904e-02,  7.6820e-03,\n",
       "          1.1597e-03, -1.7502e-02,  1.1175e-02, -1.7062e-02,  9.8273e-03,\n",
       "          2.2515e-02,  6.2307e-03, -4.3602e-03, -2.0188e-02,  3.5904e-02,\n",
       "          1.0627e-02,  1.1320e-02, -3.2941e-02, -2.1122e-02, -4.3100e-03,\n",
       "          3.1534e-02, -5.7857e-03,  2.6544e-02, -8.8975e-04,  3.4439e-02,\n",
       "         -1.3866e-02, -2.7450e-02,  2.6124e-02,  1.0444e-02,  3.7705e-02,\n",
       "          4.0174e-02, -8.3987e-03,  5.1546e-03, -2.2579e-02, -2.6712e-02,\n",
       "          1.8270e-02,  1.1330e-02, -2.1308e-02,  2.6539e-02, -1.2008e-02,\n",
       "          1.7481e-02, -9.4768e-03,  9.2125e-03,  2.1726e-03, -2.4209e-02,\n",
       "          3.7908e-02,  7.2882e-03, -2.8585e-02, -8.1739e-03, -1.8549e-02,\n",
       "          2.1407e-02,  4.7399e-03,  3.7191e-02,  4.1524e-03,  4.5340e-02,\n",
       "          2.3378e-02, -2.5337e-03, -1.5231e-02,  1.9346e-02, -2.3282e-02,\n",
       "          1.9467e-02,  2.0250e-02, -2.0982e-02, -1.4564e-03, -1.7472e-02,\n",
       "          1.1984e-02, -1.8857e-02, -1.5958e-02,  8.4577e-03, -2.9181e-02,\n",
       "          3.5551e-02, -2.6694e-02,  3.8499e-02,  2.7946e-02,  6.4884e-04,\n",
       "          5.5573e-03,  6.3948e-03,  3.6143e-02,  1.5504e-02,  3.2313e-02,\n",
       "         -1.6421e-02, -3.0610e-03,  1.5706e-02, -5.0819e-04, -6.0400e-03,\n",
       "          1.6237e-02, -1.5319e-02, -2.3983e-02, -4.1079e-02,  3.3797e-02,\n",
       "         -8.3652e-03,  1.4410e-02,  6.8869e-03, -1.0742e-03,  2.9921e-02,\n",
       "         -8.9939e-03,  1.0569e-02,  4.6661e-02, -7.5824e-03, -5.1055e-03,\n",
       "          2.8332e-03, -8.1670e-04,  1.5045e-03, -1.1923e-02, -2.3145e-02,\n",
       "          3.5167e-02,  3.6351e-02,  3.1614e-02, -2.8810e-02,  2.1281e-02,\n",
       "          3.8712e-02,  2.2693e-02,  5.9238e-03,  1.0953e-02,  2.4479e-02,\n",
       "          4.2836e-02, -5.1177e-03, -1.6670e-02,  2.9819e-02,  2.6701e-02,\n",
       "         -2.8075e-02,  3.6724e-03,  3.2077e-02, -2.4613e-02,  2.2710e-02,\n",
       "          4.1157e-03, -1.0356e-02, -7.7779e-03,  2.0666e-02, -1.1985e-02,\n",
       "          3.5271e-02,  1.5732e-02, -2.4269e-02,  1.6147e-02, -7.4688e-03,\n",
       "          1.8053e-02, -2.1999e-03, -3.9480e-03,  1.9824e-02,  2.5762e-02,\n",
       "         -3.2394e-02,  2.3120e-02, -2.6183e-02,  2.7786e-02, -2.5924e-03,\n",
       "         -6.8536e-03,  1.7545e-02, -5.2233e-03,  1.3520e-02, -3.3470e-02,\n",
       "          2.6257e-02, -2.2211e-02, -6.0935e-04, -1.5536e-02,  2.2402e-02,\n",
       "         -2.7147e-02, -6.1794e-05, -2.2400e-02, -1.7853e-02,  7.8213e-03,\n",
       "          2.7016e-02,  5.0787e-02, -2.6084e-02,  3.1671e-02,  2.5567e-03,\n",
       "         -1.5726e-02, -1.8470e-02,  2.9659e-03, -6.8463e-03,  2.7805e-03,\n",
       "          3.1082e-02,  2.4848e-02,  1.4742e-02,  3.0927e-02,  8.0623e-03,\n",
       "         -2.3183e-02,  2.1313e-02,  3.3694e-03, -9.1457e-03,  3.3248e-02,\n",
       "         -2.2263e-02,  2.0964e-02,  3.6252e-02,  1.1280e-02,  1.6990e-02,\n",
       "          3.9380e-02, -7.7271e-03,  2.7782e-02, -2.4075e-02,  6.5007e-03,\n",
       "         -4.9564e-03,  2.0443e-02,  1.7827e-02,  2.2232e-02,  3.2618e-03,\n",
       "         -1.0151e-02, -2.3544e-02,  3.3635e-02, -1.7186e-02, -1.5109e-02,\n",
       "         -2.3795e-02, -3.0025e-02, -1.1105e-02, -2.6699e-02,  2.5839e-02,\n",
       "         -8.6328e-03, -3.2910e-02,  1.2083e-02,  2.5494e-02,  4.3875e-02,\n",
       "          3.4793e-02, -1.4491e-02, -2.1099e-02,  9.0585e-03,  7.2968e-03,\n",
       "          3.3869e-02, -1.0920e-02,  3.5856e-02, -6.8463e-03,  1.1094e-02,\n",
       "          4.1191e-02, -1.9717e-02, -3.6581e-02,  1.4096e-02,  2.4800e-02,\n",
       "         -1.3483e-02, -1.0026e-02,  2.8402e-02, -3.1290e-03, -1.1108e-02,\n",
       "          1.2285e-02,  7.3828e-03, -5.4575e-03,  3.0746e-02, -8.1276e-03,\n",
       "          1.7514e-02, -2.1808e-02, -3.0145e-02, -3.0210e-02, -1.4733e-02,\n",
       "          1.0348e-03,  3.5148e-02,  3.7387e-02, -9.5711e-03,  1.7659e-02,\n",
       "          2.2078e-02,  1.2383e-02, -6.7190e-03,  2.6309e-02, -2.4468e-02,\n",
       "          3.7954e-03,  7.4744e-03, -9.2741e-03,  3.9446e-02,  3.6940e-02,\n",
       "          2.3261e-02,  1.9475e-02,  3.4452e-02, -2.1777e-02,  3.6013e-02,\n",
       "          2.7830e-02, -3.5278e-02, -1.0291e-02,  2.8338e-02,  2.9352e-02,\n",
       "         -2.7568e-03, -1.2025e-02,  3.2153e-03,  2.0549e-02, -3.0371e-02,\n",
       "          7.5733e-03, -1.9706e-02, -5.6527e-03,  7.8538e-03,  1.6083e-02,\n",
       "         -2.9464e-02,  2.4496e-02,  3.2146e-02, -2.7039e-02, -3.5280e-02,\n",
       "          1.2679e-02,  2.4723e-02,  2.6347e-02, -8.5553e-03,  4.1729e-03,\n",
       "          2.3775e-02,  6.8401e-03, -1.1133e-02,  2.1651e-02,  2.3524e-02,\n",
       "          1.5389e-02,  2.9678e-02, -1.6457e-02, -3.1522e-03,  9.7940e-03,\n",
       "         -3.3729e-02, -1.5055e-02,  3.3623e-02, -4.3105e-03,  5.6786e-03,\n",
       "          1.7137e-02, -2.0036e-02,  5.3798e-03, -2.1387e-02, -1.2064e-02,\n",
       "         -1.3133e-02, -2.7949e-02,  2.5259e-02, -1.1114e-02,  2.1406e-02,\n",
       "          3.7476e-02,  2.4415e-02, -1.0040e-02,  3.6093e-02,  2.2544e-02,\n",
       "         -3.0218e-02,  3.9368e-02,  1.5275e-02, -8.9591e-03,  3.4069e-02,\n",
       "         -1.5680e-03,  1.2180e-02, -1.1570e-02,  4.0103e-04, -1.2133e-02,\n",
       "         -2.9176e-02, -4.4094e-03,  3.5719e-03, -4.0866e-03,  1.5528e-02,\n",
       "          4.3503e-02,  6.0106e-04,  3.1338e-02,  3.4045e-02, -2.6246e-02,\n",
       "          1.4199e-02,  1.5972e-02, -1.7867e-02, -2.6637e-02,  1.1773e-02,\n",
       "          2.9847e-04,  1.2504e-02, -1.6708e-02, -1.0195e-02,  3.8374e-02,\n",
       "          2.8797e-02, -1.9606e-02, -1.0428e-02, -5.5134e-04, -2.7959e-02,\n",
       "         -5.1588e-04,  2.3639e-02, -3.0510e-02,  2.4995e-02,  9.9474e-03,\n",
       "          1.6198e-02, -1.8691e-02,  4.0303e-03,  2.5538e-02,  3.3509e-02,\n",
       "         -7.5163e-03,  3.4725e-02,  3.5494e-02,  1.4172e-02, -9.9118e-03,\n",
       "         -2.6462e-03, -9.9215e-03, -3.1669e-02, -1.2218e-02, -1.4069e-02,\n",
       "          2.8137e-02,  2.7523e-02, -3.7988e-02,  2.1175e-02, -2.2556e-02,\n",
       "         -1.0499e-02,  3.4066e-02,  2.3082e-02, -4.5225e-04,  2.5297e-02,\n",
       "          1.1493e-02, -6.1098e-03, -1.6206e-02,  2.4152e-02,  1.9817e-02,\n",
       "         -5.0711e-04,  1.9902e-02, -3.4444e-02,  2.6205e-02, -9.7492e-03,\n",
       "         -1.6943e-02, -1.6727e-02, -3.2086e-03, -6.1401e-03, -1.9812e-02,\n",
       "          4.5586e-02, -2.9077e-02, -7.0710e-03, -1.8309e-02, -1.5770e-03,\n",
       "          1.8520e-02,  3.9582e-02, -2.0202e-02,  2.0182e-03, -1.7515e-02,\n",
       "          2.0537e-02,  3.2200e-02,  1.2030e-02,  2.5209e-03, -2.4540e-02,\n",
       "         -4.7818e-03,  2.1249e-03,  3.1965e-02,  3.3822e-02, -3.3745e-03,\n",
       "         -1.8900e-02, -2.1150e-02,  2.0349e-02,  3.7766e-02, -1.9123e-02,\n",
       "          9.7176e-03,  3.0398e-02, -4.6558e-03,  3.3764e-02, -2.6239e-03,\n",
       "         -1.0506e-02, -6.1352e-03, -7.8417e-03, -2.4520e-02, -1.3460e-02,\n",
       "          2.8462e-02,  2.1051e-02, -1.5826e-04, -8.0224e-03, -1.6656e-04,\n",
       "          2.5106e-02,  1.6543e-02, -1.9367e-02, -4.4341e-03,  1.4176e-02,\n",
       "          1.2553e-02, -3.5981e-02, -2.0569e-02, -1.7158e-03,  1.0319e-02,\n",
       "          5.5122e-03, -2.0418e-02, -2.0727e-02,  2.6037e-02,  3.1281e-02,\n",
       "          2.0883e-02,  2.1008e-02, -4.8253e-04, -6.0094e-03, -3.1219e-02,\n",
       "         -8.2629e-03,  2.7962e-02,  1.1520e-02,  4.0225e-02,  2.4221e-03,\n",
       "         -3.5375e-02, -1.6445e-02,  2.2330e-02,  3.1397e-02,  4.5119e-02,\n",
       "          2.6433e-02, -3.8815e-03, -2.6665e-02, -2.1528e-02, -8.2850e-03,\n",
       "          2.5739e-02,  4.5673e-03,  3.2736e-02,  4.0909e-02, -1.4639e-02,\n",
       "         -2.0663e-02,  3.1231e-02,  9.1641e-03, -3.0191e-02, -1.4985e-03],\n",
       "        device='mps:0'),\n",
       " 'linear2.weight': tensor([[ 0.0294, -0.0134, -0.0229,  ...,  0.0040,  0.0106,  0.0283],\n",
       "         [ 0.0262, -0.0100,  0.0204,  ...,  0.0320, -0.0040, -0.0145],\n",
       "         [ 0.0068, -0.0140, -0.0253,  ...,  0.0211, -0.0032, -0.0283],\n",
       "         ...,\n",
       "         [-0.0222,  0.0110, -0.0028,  ..., -0.0266,  0.0230, -0.0001],\n",
       "         [-0.0186, -0.0128,  0.0127,  ...,  0.0120,  0.0022,  0.0221],\n",
       "         [ 0.0028,  0.0182,  0.0076,  ...,  0.0224,  0.0240,  0.0208]],\n",
       "        device='mps:0'),\n",
       " 'linear2.bias': tensor([-0.0120, -0.0005, -0.0292,  ...,  0.0262, -0.0073,  0.0268],\n",
       "        device='mps:0'),\n",
       " 'linear3.weight': tensor([[ 0.0191, -0.0288, -0.0053,  ..., -0.0061,  0.0187,  0.0101],\n",
       "         [-0.0353,  0.0009, -0.0034,  ...,  0.0183, -0.0342, -0.0221],\n",
       "         [ 0.0141,  0.0281,  0.0280,  ...,  0.0126,  0.0254,  0.0184],\n",
       "         ...,\n",
       "         [-0.0380,  0.0011,  0.0111,  ...,  0.0018, -0.0104, -0.0300],\n",
       "         [ 0.0018,  0.0087,  0.0009,  ...,  0.0188,  0.0107, -0.0342],\n",
       "         [ 0.0092,  0.0050, -0.0444,  ..., -0.0142, -0.0252, -0.0373]],\n",
       "        device='mps:0'),\n",
       " 'linear3.bias': tensor([-0.0100,  0.0149,  0.0165, -0.0257,  0.0193,  0.0282,  0.0004,  0.0231,\n",
       "          0.0002, -0.0037], device='mps:0')}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 313/313 [00:05<00:00, 54.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.954\n",
      "wrong counts for the digit 0: 19\n",
      "wrong counts for the digit 1: 15\n",
      "wrong counts for the digit 2: 56\n",
      "wrong counts for the digit 3: 39\n",
      "wrong counts for the digit 4: 49\n",
      "wrong counts for the digit 5: 54\n",
      "wrong counts for the digit 6: 39\n",
      "wrong counts for the digit 7: 58\n",
      "wrong counts for the digit 8: 73\n",
      "wrong counts for the digit 9: 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# testing the model to see on which digit the model is struggling\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    wrong_counts = [0]*10\n",
    "    for i in range(10):\n",
    "        wrong_counts[i] = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader, desc='Testing'):\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(x.view(-1, 784))\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct +=1\n",
    "                else:\n",
    "                    wrong_counts[y[idx]] +=1\n",
    "                total +=1\n",
    "    print(f'Accuracy: {round(correct/total, 3)}')\n",
    "    for i in range(len(wrong_counts)):\n",
    "        print(f'wrong counts for the digit {i}: {wrong_counts[i]}')\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our model is the most wrong for digit 8, so will fine tune our small model on digit 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no of parameters are: 2813804\n",
      "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000])\n",
      "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000])\n",
      "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# total no of parameters in our model \n",
    "def get_num_params(model):\n",
    "  return sum(p.numel() for p in model.parameters())\n",
    "print('total no of parameters are:',get_num_params(model))\n",
    "\n",
    "total_parameters_original = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# no of parameters layer wise \n",
    "for index, layer in enumerate([model.linear, model.linear2, model.linear3]):\n",
    "    print(f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self, features_in, features_out, r=1, alpha=1, device = device):\n",
    "        super(LoRA, self).__init__()\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scale = self.alpha / self.r\n",
    "        self.lora_A = nn.Parameter(torch.zeros((r,features_out)).to(device))\n",
    "        self.lora_B = nn.Parameter(torch.zeros((features_in, r)).to(device))\n",
    "        nn.init.normal_(self.lora_A, mean=0, std=1) \n",
    "        self.enabled = True \n",
    "\n",
    "    def forward(self, original_weights):\n",
    "        if self.enabled:\n",
    "            # Return W + (B*A)*scale\n",
    "            return original_weights + torch.matmul(self.lora_B, self.lora_A).view(original_weights.shape) * self.scale\n",
    "        else:\n",
    "            return original_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParametrizedLinear(\n",
       "  in_features=2000, out_features=10, bias=True\n",
       "  (parametrizations): ModuleDict(\n",
       "    (weight): ParametrizationList(\n",
       "      (0): LoRA()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the parameterization \n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "\n",
    "def linear_layer_parameterization(layer, device, lora_alpha=1):\n",
    "    # Only add the parameterization to the weight matrix, ignore the Bias\n",
    "    features_in, features_out = layer.weight.shape\n",
    "    return LoRA(\n",
    "        features_in, features_out, r=1, alpha=lora_alpha, device=device\n",
    "    )\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    model.linear, \"weight\", linear_layer_parameterization(model.linear, device)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    model.linear2, \"weight\", linear_layer_parameterization(model.linear2, device)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    model.linear3, \"weight\", linear_layer_parameterization(model.linear3, device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000]) + Lora_A: torch.Size([1, 784]) + Lora_B: torch.Size([1000, 1])\n",
      "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000]) + Lora_A: torch.Size([1, 1000]) + Lora_B: torch.Size([2000, 1])\n",
      "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10]) + Lora_A: torch.Size([1, 2000]) + Lora_B: torch.Size([10, 1])\n",
      "Total number of parameters (original): 2,807,010\n",
      "Total number of parameters (original + LoRA): 2,813,804\n",
      "Parameters introduced by LoRA: 6,794\n",
      "Parameters incremment: 0.242%\n"
     ]
    }
   ],
   "source": [
    "total_parameters_lora = 0\n",
    "total_parameters_non_lora = 0\n",
    "for index, layer in enumerate([model.linear, model.linear2, model.linear3]):\n",
    "    total_parameters_lora += layer.parametrizations[\"weight\"][0].lora_A.nelement() + layer.parametrizations[\"weight\"][0].lora_B.nelement()\n",
    "    total_parameters_non_lora += layer.weight.nelement() + layer.bias.nelement()\n",
    "    print(\n",
    "        f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape} + Lora_A: {layer.parametrizations[\"weight\"][0].lora_A.shape} + Lora_B: {layer.parametrizations[\"weight\"][0].lora_B.shape}'\n",
    "    )\n",
    "# The non-LoRA parameters count must match the original network\n",
    "# assert total_parameters_non_lora == total_parameters_original\n",
    "print(f'Total number of parameters (original): {total_parameters_non_lora:,}')\n",
    "print(f'Total number of parameters (original + LoRA): {total_parameters_lora + total_parameters_non_lora:,}')\n",
    "print(f'Parameters introduced by LoRA: {total_parameters_lora:,}')\n",
    "parameters_incremment = (total_parameters_lora / total_parameters_non_lora) * 100\n",
    "print(f'Parameters incremment: {parameters_incremment:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing non-LoRA parameter linear.bias\n",
      "Freezing non-LoRA parameter linear.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter linear2.bias\n",
      "Freezing non-LoRA parameter linear2.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter linear3.bias\n",
      "Freezing non-LoRA parameter linear3.parametrizations.weight.original\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# freezing the non lora parameters as the paper says cause we no longer need them \n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' not in name:\n",
    "        print(f'Freezing non-LoRA parameter {name}')\n",
    "        param.requires_grad = False\n",
    "\n",
    "# loading the dataseat again and now focusing only on the digit 8\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "exclude_indices = mnist_trainset.targets == 8\n",
    "mnist_trainset.data = mnist_trainset.data[exclude_indices]\n",
    "mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n",
    "# Create a dataloader for the training\n",
    "train_loader = DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Train the network with LoRA only on the digit 9 and only for 100 batches (hoping that it would improve the performance on the digit 8)\n",
    "print(train(train_loader=train_loader, model=model, num_epochs=1, device=device, total_iterations_limit=100, criterion=criterion))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the frozen parameters are still unchanged by the finetuning\n",
    "assert torch.all(model.linear.parametrizations.weight.original == original_weights['linear.weight'])\n",
    "assert torch.all(model.linear2.parametrizations.weight.original == original_weights['linear2.weight'])\n",
    "assert torch.all(model.linear3.parametrizations.weight.original == original_weights['linear3.weight'])\n",
    "\n",
    "enable_disable_lora(enabled=True)\n",
    "# The new linear1.weight is obtained by the \"forward\" function of our LoRA parametrization\n",
    "# The original weights have been moved to model.linear.parametrizations.weight.original\n",
    "# More info here: https://pytorch.org/tutorials/intermediate/parametrizations.html#inspecting-a-parametrized-module\n",
    "assert torch.equal(model.linear.weight, model.linear.parametrizations.weight.original + (model.linear.parametrizations.weight[0].lora_B @ model.linear.parametrizations.weight[0].lora_A) * model.linear.parametrizations.weight[0].scale)\n",
    "\n",
    "enable_disable_lora(enabled=False)\n",
    "# If we disable LoRA, the linear1.weight is the original one\n",
    "assert torch.equal(model.linear.weight, original_weights['linear1.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with LoRA enabled\n",
    "enable_disable_lora(enabled=True)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with LoRA disabled\n",
    "enable_disable_lora(enabled=False)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_central_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
